{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10952bb",
   "metadata": {},
   "source": [
    "## Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dcb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CCP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from utils import (sentences, train_data, val_data, english_vectorizer, french_vectorizer,\n",
    "                   masked_loss, masked_acc, tokens_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0b421",
   "metadata": {},
   "source": [
    "### Lets look at a random sentence and its equivalent translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c45b2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (to translate) sentence:\n",
      "\n",
      "\"Top-down economics never works,\" said Obama. \"The country does not succeed when just those at the very top are doing well. We succeed when the middle class gets bigger, when it feels greater security.\"\n",
      "\n",
      "French (translation) sentence:\n",
      "\n",
      "« L'économie en partant du haut vers le bas, ça ne marche jamais, » a dit Obama. « Le pays ne réussit pas lorsque seulement ceux qui sont au sommet s'en sortent bien. Nous réussissons lorsque la classe moyenne s'élargit, lorsqu'elle se sent davantage en sécurité. »\n",
      "\n"
     ]
    }
   ],
   "source": [
    "french_sentences, english_sentences = sentences\n",
    "\n",
    "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
    "print(f\"French (translation) sentence:\\n\\n{french_sentences[-5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055f85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del english_sentences\n",
    "del french_sentences\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d41001",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edaa3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words of the English vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'i', 'you', 'to', 'the', '?']\n",
      "\n",
      "First 10 words of the French vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'je', 'de', 'a', '?', 'pas']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 10 words of the English vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
    "print(f\"First 10 words of the French vocabulary:\\n\\n{french_vectorizer.get_vocabulary()[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf24084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The French vocabulary is made up of 12000 words.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = french_vectorizer.vocabulary_size()\n",
    "\n",
    "print(f\"The French vocabulary is made up of {vocab_size} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19fdd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers to convert words to ids and vice-versa\n",
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary=french_vectorizer.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary=french_vectorizer.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\",\n",
    "    invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50869e6",
   "metadata": {},
   "source": [
    "#### Trying out the above functions to some special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de8f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id for the [UNK] token is 1\n",
      "The id for the [SOS] token is 2\n",
      "The id for the [EOS] token is 3\n",
      "The id for the lorsque (when) token is 301\n"
     ]
    }
   ],
   "source": [
    "unk_id = word_to_id(\"[UNK]\")\n",
    "sos_id = word_to_id(\"[SOS]\")\n",
    "eos_id = word_to_id(\"[EOS]\")\n",
    "lorsque_id = word_to_id(\"lorsque\")\n",
    "\n",
    "print(f\"The id for the [UNK] token is {unk_id}\")\n",
    "print(f\"The id for the [SOS] token is {sos_id}\")\n",
    "print(f\"The id for the [EOS] token is {eos_id}\")\n",
    "print(f\"The id for the lorsque (when) token is {lorsque_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2f750",
   "metadata": {},
   "source": [
    "Now lets take a look on the actual tokenized data to be fed into the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947312d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized English sentence:\n",
      "[  2  17 252  20  10 147  55   7  61 254   7   8 778   4   3   0   0]\n",
      "\n",
      "\n",
      "Tokenized French shifted right sentence:\n",
      "[   2 5229   47  647   42  219   19  691  218    4    0    0    0    0\n",
      "    0    0    0    0]\n",
      "\n",
      "\n",
      "Tokenized French output sentence:\n",
      "[5229   47  647   42  219   19  691  218    4    3    0    0    0    0\n",
      "    0    0    0    0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (to_translate, sr_translation), translation in train_data.take(1):\n",
    "    print(f\"Tokenized English sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized French shifted right sentence:\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized French output sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae8e9d",
   "metadata": {},
   "source": [
    "### NMT Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eade3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddd157",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8157dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class encoder inherits from tensorflow keras layers class\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"\n",
    "        vocab_size : size of the vocabulary\n",
    "        units : Number of units in the LSTM Layer\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        \n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='sum',\n",
    "            layer=tf.keras.layers.LSTM(\n",
    "                units=units,\n",
    "                return_sequences=True\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def call(self, context):\n",
    "        \"\"\"\n",
    "        context : The sentence to translate\n",
    "        \"\"\"\n",
    "        # passing the context through embedding layer\n",
    "        x = self.embedding(context)\n",
    "        \n",
    "        # passing the embedding layer through rnn\n",
    "        x = self.rnn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d8fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences in english has shape: (64, 17)\n",
      "\n",
      "Encoder output has shape: (64, 17, 256)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating an instance of the Encoder class\n",
    "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "# passing the english (context) sentence to the encoder\n",
    "encoder_output = encoder(to_translate)\n",
    "\n",
    "print(f\"Tensor of sentences in english has shape: {to_translate.shape}\\n\")\n",
    "print(f\"Encoder output has shape: {encoder_output.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b0379",
   "metadata": {},
   "source": [
    "### CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd6f0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a cross-attention class\n",
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        units : number of units in the LSTM Layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = (\n",
    "            tf.keras.layers.MultiHeadAttention(\n",
    "                key_dim=units,\n",
    "                num_heads=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "    def call(self, context, target):\n",
    "        \"\"\"\n",
    "        context : The sentence to translate\n",
    "        target : The equivalent shifted right translation\n",
    "        \"\"\"\n",
    "        # calling the multi-head attention by passing query and key value\n",
    "        attn_output = self.mha(\n",
    "            query=target,\n",
    "            value=context\n",
    "        )\n",
    "            \n",
    "        x = self.add([target, attn_output])\n",
    "            \n",
    "        x = self.layernorm(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cafbc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of context has shape: (64, 17, 256)\n",
      "\n",
      "Tensor of sentences in french (shifted-right) has shape: (64, 18, 256)\n",
      "\n",
      "Tensor of attention scores has shape: (64, 18, 256)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instanciating cross-attention class\n",
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# embedding the shifted-right translation\n",
    "sr_translation_embedded = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
    "\n",
    "# finally getting the attention scores\n",
    "attention_result = attention_layer(encoder_output, sr_translation_embedded)\n",
    "\n",
    "print(f\"Tensor of context has shape: {encoder_output.shape}\\n\")\n",
    "print(f\"Tensor of sentences in french (shifted-right) has shape: {sr_translation_embedded.shape}\\n\")\n",
    "print(f\"Tensor of attention scores has shape: {attention_result.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b0172",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4afb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a class Decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"\n",
    "        vocab_size : size of the vocabulary\n",
    "        units : Number of units in the LSTM Layer\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # The embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        \n",
    "        # The pre-attention RNN\n",
    "        self.pre_attn_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )\n",
    "        \n",
    "        # The attention layer\n",
    "        self.attention = CrossAttention(units)\n",
    "        \n",
    "        # The post-attention RNN\n",
    "        self.post_attn_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True\n",
    "        )\n",
    "        \n",
    "        # The final dense layer\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=vocab_size,\n",
    "            activation=tf.nn.log_softmax\n",
    "        )\n",
    "        \n",
    "    def call(self, context, target, state=None, return_state=False):\n",
    "        \"\"\"\n",
    "        context : The sentence to translate\n",
    "        target : The equivalent shifted right translation\n",
    "        state : Hidden states of the pre-attention LSTM\n",
    "        return_state : Set it to true if want to return the hidden states\n",
    "        \"\"\"\n",
    "        # embedding of the input (target)\n",
    "        x = self.embedding(target)\n",
    "        \n",
    "        # embedded input through pre-attention LSTM\n",
    "        x, hidden_state, cell_state = self.pre_attn_rnn(x, initial_state=state)\n",
    "        \n",
    "        # performing cross-attention between the context and the target\n",
    "        x = self.attention(context, x)\n",
    "        \n",
    "        # passing the attention through post-attention LSTM\n",
    "        x = self.post_attn_rnn(x)\n",
    "        \n",
    "        # finally the logits\n",
    "        logits = self.output_layer(x)\n",
    "        \n",
    "        if return_state:\n",
    "            return logits, [hidden_state, cell_state]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d892147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context has shape: (64, 17, 256)\n",
      "The target (shifted-right) has shape: (64, 18)\n",
      "The logits has shape: (64, 18, 12000)\n"
     ]
    }
   ],
   "source": [
    "# instanciating the decoder class\n",
    "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "# computing the logits from the decoder\n",
    "logits = decoder(encoder_output, sr_translation)\n",
    "\n",
    "print(f\"The context has shape: {encoder_output.shape}\")\n",
    "print(f\"The target (shifted-right) has shape: {sr_translation.shape}\")\n",
    "print(f\"The logits has shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0909941",
   "metadata": {},
   "source": [
    "### Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9de3b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the final Translator class\n",
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"\n",
    "        vocab_size : size of the vocabulary\n",
    "        units : Number of units in the LSTM Layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # defining the encoder with vocab size and units\n",
    "        self.encoder = Encoder(vocab_size, units)\n",
    "        \n",
    "        # defining the decoder with vocab size and units\n",
    "        self.decoder = Decoder(vocab_size, units)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : Tuple containing context and target (shifted-right)\n",
    "        \"\"\"\n",
    "        # unpacking the inputs tuple\n",
    "        context, target = inputs\n",
    "        \n",
    "        # passing the context through the encoder\n",
    "        encoded_context = self.encoder(context)\n",
    "        \n",
    "        # getting the logits by passing encoded context and target to the decoder\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da4950c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context has shape: (64, 17)\n",
      "The target (shifted-right) has shape: (64, 18)\n",
      "The logits of translator output has shape: (64, 18, 12000)\n"
     ]
    }
   ],
   "source": [
    "# instanciating Translator class\n",
    "translator = Translator(VOCAB_SIZE, UNITS)\n",
    "\n",
    "# computing the logits for every word in vocabulary\n",
    "logits = translator((to_translate, sr_translation))\n",
    "\n",
    "print(f\"The context has shape: {to_translate.shape}\")\n",
    "print(f\"The target (shifted-right) has shape: {sr_translation.shape}\")\n",
    "print(f\"The logits of translator output has shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51523072",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9af7180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_data.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=50,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "    )\n",
    "    \n",
    "    model.save_weights(\"NMT_model.h5\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "887bce68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 317s 575ms/step - loss: 1.8791 - masked_acc: 0.3783 - masked_loss: 3.9247 - val_loss: 1.6121 - val_masked_acc: 0.4453 - val_masked_loss: 3.3268\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 278s 557ms/step - loss: 1.4370 - masked_acc: 0.4875 - masked_loss: 3.0157 - val_loss: 1.2854 - val_masked_acc: 0.5366 - val_masked_loss: 2.6251\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 279s 558ms/step - loss: 1.1849 - masked_acc: 0.5634 - masked_loss: 2.4845 - val_loss: 1.0398 - val_masked_acc: 0.6043 - val_masked_loss: 2.1761\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 276s 551ms/step - loss: 1.0408 - masked_acc: 0.6109 - masked_loss: 2.1496 - val_loss: 0.9348 - val_masked_acc: 0.6400 - val_masked_loss: 1.9585\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 281s 562ms/step - loss: 0.8782 - masked_acc: 0.6478 - masked_loss: 1.8604 - val_loss: 0.8865 - val_masked_acc: 0.6534 - val_masked_loss: 1.8102\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 280s 559ms/step - loss: 0.7804 - masked_acc: 0.6744 - masked_loss: 1.6503 - val_loss: 0.8004 - val_masked_acc: 0.6723 - val_masked_loss: 1.6608\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 278s 555ms/step - loss: 0.7462 - masked_acc: 0.6868 - masked_loss: 1.5667 - val_loss: 0.7332 - val_masked_acc: 0.6841 - val_masked_loss: 1.5721\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 280s 560ms/step - loss: 0.7112 - masked_acc: 0.6967 - masked_loss: 1.4943 - val_loss: 0.6892 - val_masked_acc: 0.6981 - val_masked_loss: 1.4870\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 279s 559ms/step - loss: 0.6651 - masked_acc: 0.7098 - masked_loss: 1.3966 - val_loss: 0.6956 - val_masked_acc: 0.7105 - val_masked_loss: 1.4028\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 277s 554ms/step - loss: 0.5875 - masked_acc: 0.7331 - masked_loss: 1.2202 - val_loss: 0.6561 - val_masked_acc: 0.7142 - val_masked_loss: 1.3906\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 284s 567ms/step - loss: 0.5651 - masked_acc: 0.7355 - masked_loss: 1.2108 - val_loss: 0.6395 - val_masked_acc: 0.7257 - val_masked_loss: 1.3112\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 279s 559ms/step - loss: 0.5630 - masked_acc: 0.7381 - masked_loss: 1.1824 - val_loss: 0.6332 - val_masked_acc: 0.7260 - val_masked_loss: 1.3043\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 279s 557ms/step - loss: 0.5560 - masked_acc: 0.7418 - masked_loss: 1.1620 - val_loss: 0.5977 - val_masked_acc: 0.7339 - val_masked_loss: 1.2547\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 279s 557ms/step - loss: 0.4940 - masked_acc: 0.7614 - masked_loss: 1.0295 - val_loss: 0.6045 - val_masked_acc: 0.7355 - val_masked_loss: 1.2404\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 281s 561ms/step - loss: 0.4675 - masked_acc: 0.7677 - masked_loss: 0.9893 - val_loss: 0.6136 - val_masked_acc: 0.7324 - val_masked_loss: 1.2705\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 279s 559ms/step - loss: 0.4724 - masked_acc: 0.7671 - masked_loss: 0.9946 - val_loss: 0.5760 - val_masked_acc: 0.7395 - val_masked_loss: 1.2199\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 280s 559ms/step - loss: 0.4756 - masked_acc: 0.7660 - masked_loss: 0.9948 - val_loss: 0.5596 - val_masked_acc: 0.7366 - val_masked_loss: 1.2202\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 278s 556ms/step - loss: 0.4525 - masked_acc: 0.7745 - masked_loss: 0.9474 - val_loss: 0.5785 - val_masked_acc: 0.7454 - val_masked_loss: 1.2040\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 280s 560ms/step - loss: 0.4022 - masked_acc: 0.7920 - masked_loss: 0.8461 - val_loss: 0.5437 - val_masked_acc: 0.7560 - val_masked_loss: 1.1272\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 278s 556ms/step - loss: 0.4129 - masked_acc: 0.7865 - masked_loss: 0.8658 - val_loss: 0.5611 - val_masked_acc: 0.7455 - val_masked_loss: 1.1631\n"
     ]
    }
   ],
   "source": [
    "# Training the translator\n",
    "\n",
    "trained_translator, history = compile_and_train(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f1d40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, embedding_4_layer_call_fn, embedding_4_layer_call_and_return_conditional_losses, cross_attention_2_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: NMT_Model_trained\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: NMT_Model_trained\\assets\n"
     ]
    }
   ],
   "source": [
    "trained_translator.save(\"NMT_Model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09403888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"translator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  4122624   \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  7470304   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,592,928\n",
      "Trainable params: 11,592,928\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "translator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48cb3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.load_weights(\"NMT_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07139f50",
   "metadata": {},
   "source": [
    "The shape of weights of the last log_softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eed0d603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.layers[-1].get_weights()[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb6cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_training(model, epochs=2, steps_per_epoch=500):\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_data.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=50,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "    )\n",
    "    \n",
    "    model.save_weights(\"NMT_model_chkpt_1.h5\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51f363f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "500/500 [==============================] - 326s 581ms/step - loss: 0.3928 - masked_acc: 0.7980 - masked_loss: 0.8210 - val_loss: 0.5495 - val_masked_acc: 0.7554 - val_masked_loss: 1.1211\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 277s 553ms/step - loss: 0.4106 - masked_acc: 0.7915 - masked_loss: 0.8562 - val_loss: 0.5522 - val_masked_acc: 0.7534 - val_masked_loss: 1.1398\n"
     ]
    }
   ],
   "source": [
    "trained_translator, history = resume_training(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c9c2c",
   "metadata": {},
   "source": [
    "### Using the model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405a247",
   "metadata": {},
   "source": [
    "Loading the trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f275fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.load_weights(\"NMT_model_chkpt_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c73860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
    "    \"\"\"\n",
    "    decoder : The decoder\n",
    "    context : Encoded sentence to translate\n",
    "    next_token : The predicted next_token\n",
    "    done : True if translation is complete\n",
    "    state : Hidden states of the pre-attention LSTM layer\n",
    "    temperature : Controls randomness of the predicted tokens\n",
    "    \"\"\"\n",
    "    # getting the logits and state from the decoder\n",
    "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
    "    \n",
    "    # getting the last predicted logit\n",
    "    logits = logits[:, -1, :]\n",
    "    \n",
    "    # if temperature is 0.0 take the argmax of logits\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "        \n",
    "    # if temperature is not 0.0 take the next_token sampled out of logits\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "        \n",
    "    # trimming the dimension of size 1\n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "    \n",
    "    # getting the logit of the selected next_token\n",
    "    logit = logits[next_token].numpy()\n",
    "    \n",
    "    # reshaping the next token to (1, 1)\n",
    "    next_token = tf.reshape(next_token, shape=(1, 1))\n",
    "    \n",
    "    # if the next token is \"eos\" setting done to true\n",
    "    if next_token == eos_id:\n",
    "        done = True\n",
    "        \n",
    "    return next_token, logit, state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0807cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [[9980]]\n",
      "Logit: -18.9135\n",
      "Done? False\n"
     ]
    }
   ],
   "source": [
    "# A sentence to translate\n",
    "eng_sentence = \"I love languages\"\n",
    "\n",
    "# Converting the sentence to a tensor\n",
    "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
    "\n",
    "# vectorizing it and passing it through the encoder\n",
    "context = english_vectorizer(texts).to_tensor()\n",
    "context = encoder(context)\n",
    "\n",
    "# The first token should be \"SOS\"\n",
    "next_token = tf.fill((1, 1), sos_id)\n",
    "\n",
    "# Hidden and cell states of the LSTM mocked using uniform samples\n",
    "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
    "\n",
    "# Not done until the next token is \"EOS\"\n",
    "done = False\n",
    "\n",
    "# Generating next token\n",
    "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
    "\n",
    "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387d242",
   "metadata": {},
   "source": [
    "### Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d88d1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform translation\n",
    "def translate(model, text, max_length=50, temperature=0.0):\n",
    "    \"\"\"\n",
    "    model : The trained translator\n",
    "    text : The sentence to translate\n",
    "    max_length : The maximum length of the translation\n",
    "    temperature : Controls randomness of the predicted tokens\n",
    "    \"\"\"\n",
    "    # list to save tokens and logits\n",
    "    tokens, logits = [], []\n",
    "    \n",
    "    # converting the original text to a tensor\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    \n",
    "    # vectorizing the text\n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "    \n",
    "    # passing through encoder\n",
    "    context = model.encoder(context)\n",
    "    \n",
    "    # The first token should be \"SOS\"\n",
    "    next_token = tf.fill((1, 1), sos_id)\n",
    "    \n",
    "    # # Hidden and cell states of the LSTM should be tensors of zeros\n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "    \n",
    "    # Not done until the next token is \"EOS\"\n",
    "    done = False\n",
    "    \n",
    "    # iterating for length max_length\n",
    "    for _ in range(max_length):\n",
    "        \n",
    "        # generating next token\n",
    "        next_token, logit, state, done = generate_next_token(\n",
    "            decoder=model.decoder,\n",
    "            context=context,\n",
    "            next_token=next_token,\n",
    "            done=done,\n",
    "            state=state,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # if done is true breaking the loop\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "        # adding the next token to the list\n",
    "        tokens.append(next_token)\n",
    "        \n",
    "        # adding the logit to the list\n",
    "        logits.append(logit)\n",
    "        \n",
    "    # concating all tokens into a tensor\n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "    \n",
    "    # converting the translated tokens into texts\n",
    "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
    "    translation = translation.numpy().decode()\n",
    "    \n",
    "    return translation, logits[-1], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7efd187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : England is a wonderful country. I would love to visit it some day!\n",
      "\n",
      "Translation : langleterre est un pays merveilleux . je [UNK] seul un jour !\n",
      "\n",
      "logit : -0.2357894629240036\n",
      "\n",
      "Tokens : [[2997   18   19  483 1871    4    5    1  182   19  216   34]]\n"
     ]
    }
   ],
   "source": [
    "temp = 0.0\n",
    "sentence = \"England is a wonderful country. I would love to visit it some day!\"\n",
    "\n",
    "translation, logit, tokens = translate(translator, sentence, temperature=temp)\n",
    "\n",
    "print(f\"Original sentence : {sentence}\\n\\nTranslation : {translation}\\n\\nlogit : {logit}\\n\\nTokens : {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75396b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
